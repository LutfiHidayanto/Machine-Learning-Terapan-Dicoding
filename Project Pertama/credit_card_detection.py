# -*- coding: utf-8 -*-
"""Yet another copy of credit_card_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aJe5_Ht9kteLBz-Od8A6nrRWa392TCSm

Nama : Hajid Lutfi Hidayanto

Domisili : Surakarta

# Data Collecting

Proyek ini berjudul "credit card fraud detection" yang berguna untuk mendeteksi penipuan kartu kredit. Algoritma yang akan digunakan antara lain, decision tree, random forest, dan neural network.

Link dataset: https://www.kaggle.com/datasets/nelgiriyewithana/credit-card-fraud-detection-dataset-2023
"""

from google.colab import userdata
import os

os.environ["KAGGLE_KEY"] = userdata.get('KAGGLE_KEY')
os.environ["KAGGLE_USERNAME"] = userdata.get('KAGGLE_USERNAME')

!kaggle datasets download -d nelgiriyewithana/credit-card-fraud-detection-dataset-2023

!unzip credit-card-fraud-detection-dataset-2023

"""# Importing Library"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.decomposition import PCA

"""# Exploratory Data Analysis

"""

df = pd.read_csv('/content/creditcard_2023.csv')
df

"""Pada dataset ini, terdapat **31** kolom terdiri dari **29** features dan **1** label"""

df.count()

"""**Tidak** ada data **null** dalam dataset"""

df.info()

"""Dataset mempunyai data **kuantitatif** terdiri dari **float64** untuk seluruh features dan **int64** untuk label"""

sns.countplot(data=df, x='Class')
print(df['Class'].value_counts())

"""Dataset termasuk dataset yang sangat **balance** mempunyai **284315** sampel untuk masing-masing kelas

Penjelasan label:

1: Fraud

2: Non-fraud
"""

df.describe().round(5)

"""## Correlation Analysis"""

plt.figure(figsize=(22, 22))
corr_matrices = df.corr()

sns.heatmap(data=corr_matrices, cmap='coolwarm', annot=True)
plt.title('Correlation Matrices')
plt.show()

corr_matrices.drop(columns=['id', 'Class'], inplace=True, errors='ignore')
corr_matrices.drop(index=['id', 'Class'], inplace=True, errors='ignore')
corr_matrices

corr_values = corr_matrices.unstack().sort_values(ascending=False)

corr_values = corr_values[corr_values != 1]
corr_values.drop_duplicates(inplace=True)
corr_values

for i in range(len(corr_values)):
    if corr_values[i] < 0.7:
      break
    X, y = corr_values.index[i]
    sns.scatterplot(data=df, x=X, y=y, hue='Class')
    plt.show()

"""Terdapat **9** correlation set yang lebih dari 0.7, maka kita akan coba gunakan **PCA** untuk mereduksi dimensi fitur

## Dataset Spliting & Scaling

### Spliting dataset
"""

X = df.drop(columns=['id', 'Class'])
y = df['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)

"""Karena jumlah dataset yang banyak, maka dataset dibagi **85%** untuk training dan **15%** untuk testing"""

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""### Scaling dataset"""

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""## Reducing Feature Dimension Using PCA"""

pca = PCA(0.95)
pca.fit(X_train)
X_train = pca.transform(X_train)
X_test = pca.transform(X_test)
print(f"Shape of training data: {X_train.shape}")
print(f"Shape of testing data: {X_test.shape}")

"""Menggunakan **95%** Variance Threshold untuk PCA. Dengan ini, kita berhasil mengubah dimensi fitur yang sebelumnya **29** menjadi **22**"""

models_accuracy = pd.DataFrame(index=['train', 'test'],
                      columns=['DT', 'NN', 'RF'])

"""Membuat dataframe untuk menyimpan akurasi tiap model

# Model Training

## Decision Tree
"""

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

dt_predict_train=dt.predict(X_train)
dt_predict_test=dt.predict(X_test)

dt_accuracy_train = accuracy_score(y_true=y_train, y_pred=dt_predict_train)
dt_accuracy_test = accuracy_score(y_true=y_test, y_pred=dt_predict_test)

models_accuracy.loc['train', 'DT'] = dt_accuracy_train
models_accuracy.loc['test', 'DT'] = dt_accuracy_test

print(classification_report(y_true=y_test, y_pred=dt_predict_test))

print(models_accuracy[['DT']])

"""Didapatkan hasil akurasi yang bagus dari decision tree, mencapai **0.998..** untuk testing

## Random Forest
"""

rf = RandomForestClassifier(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
rf.fit(X_train, y_train)

rf_predict_train = rf.predict(X_train)
rf_predict_test = rf.predict(X_test)

rf_accuracy_train = accuracy_score(y_true=y_train, y_pred=rf_predict_train)
rf_accuracy_test = accuracy_score(y_true=y_test, y_pred=rf_predict_test)

models_accuracy.loc['train', 'RF'] = rf_accuracy_train
models_accuracy.loc['test', 'RF'] = rf_accuracy_test

print(classification_report(y_true=y_test, y_pred=rf_predict_test))

print(models_accuracy[['RF']])

"""Didapatkan hasil yang lebih baik dari decision tree, akurasi random forest mencapai **0.9996..** untuk testing

## Neural Network Model
"""

target_accuracy = 0.9998

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > target_accuracy and logs.get('val_accuracy') > target_accuracy):
      print(f"\nAkurasi telah mencapai > {target_accuracy * 100}%!")
      self.model.stop_training = True
callbacks = myCallback()

y_train = np.asarray(y_train).astype('int64').reshape((-1,1))
y_test = np.asarray(y_test).astype('int64').reshape((-1,1))

nn = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(None, X_train.shape[1])),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

nn.compile(
    optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

nn.summary()

nn_history = nn.fit(X_train, y_train,
                batch_size=512,
                epochs=10,
                validation_data=(X_test, y_test),
                callbacks=[callbacks],
                verbose=2)

nn_accuracy_train = nn_history.history['accuracy'][-1]
nn_accuracy_test = nn_history.history['val_accuracy'][-1]

models_accuracy.loc['train', 'NN'] = nn_accuracy_train
models_accuracy.loc['test', 'NN'] = nn_accuracy_test

plt.plot(nn_history.history['accuracy'])
plt.plot(nn_history.history['val_accuracy'])

plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'])
plt.show()

def model_results(model):
    pred = model.predict(X_test)
    pred = np.where(pred > 0.5, 1, 0)
    print(classification_report(y_test, pred))

model_results(nn)

"""Didapatkan hasil yang baik tetapi tidak lebih baik daripada random forest"""

print(models_accuracy)