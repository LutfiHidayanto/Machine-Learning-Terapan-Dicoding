# -*- coding: utf-8 -*-
"""Copy of RecommendationSystemDrama.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KzdT3IgubF7LH6ESXGpjhxRjcYJGwlpQ

# MyDramalist.com Recommender System  
Author: Hajid Lutfi Hidayanto  
Github: [LutfiHidayanto](https://github.com/LutfiHidayanto)

## Downloading Dataset

Link Dataset: [Kaggle](https://www.kaggle.com/datasets/amarang/mydramalist-ratings)
"""

from google.colab import userdata
import os

os.environ["KAGGLE_KEY"] = userdata.get('KAGGLE_KEY')
os.environ["KAGGLE_USERNAME"] = userdata.get('KAGGLE_USERNAME')

!kaggle datasets download -d amarang/mydramalist-ratings


!unzip mydramalist-ratings

"""## Importing Library"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from scipy.sparse import csr_matrix
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""## Exploratory Data Analysis & Data Preprocessing"""

pairs = pd.read_parquet('/content/df_pairs.parquet')
shows = pd.read_parquet('/content/df_shows.parquet')
users = pd.read_parquet('/content/df_users.parquet')

print(f"Jumlah data pairs: {len(pairs)}")
print(f"Jumlah data shows: {len(shows)}")
print(f"Jumlah data users: {len(users)}")

"""### shows"""

shows.head()

shows.info()

"""Terdapat 3 data kategorikal (country, kind, title) dan 2 data numerik (year, sid)"""

shows.isnull().sum()

"""Tidak ada data null dalam shows"""

pd.unique(shows['kind'].to_list())

duplicates_mask = shows['sid'].duplicated()
sid_with_duplicates = shows['sid'][duplicates_mask]

print(sid_with_duplicates)
print(f"Banyak duplicates: {len(sid_with_duplicates)}")

duplicated_entries = shows[shows['sid'].isin(sid_with_duplicates)].sort_values(by='sid')
print(len(duplicated_entries))
duplicated_entries

"""Mengecek di situs mydramalist.com untuk menentukan data yang benar"""

shows = shows.drop(index=[18087, 4437, 16152, 31065, 22576, 13059, 37990, 29451, 29450, 13061, 49038, 13062, 18249, 13071, 17114, 34194, 8698, 39719])

sns.histplot(x='kind', data=shows, palette="Set3", hue="kind")
plt.title("Perbandingan jumlah drama berdasarkan tipe drama di mydramalist.com")
plt.show()

shows[shows['kind'] == 'TV Show']

plt.figure(figsize=(14, 6))
sns.histplot(x='country', data=shows, palette="Set3", hue="country")
plt.title("Perbandingan jumlah drama berdasarkan negara asal di mydramalist.com")
plt.show()

"""Melihat shows dari singapore"""

shows[shows['country'] == 'Singapore']

duplicates_mask = shows['sid'].duplicated()
sid_with_duplicates = shows['sid'][duplicates_mask]

print(sid_with_duplicates)
print(f"Banyak duplicates: {len(sid_with_duplicates)}")

shows['kind country'] = shows['kind'].astype(str) + " " + shows['country'].astype(str)
shows.head()

"""## users"""

users.head()

users.info()

users.isnull().sum()

"""Terdapat null di dalam dataset tetapi tidak masalah karena fitur-fitur yang memiliki null tidak digunakan (join_date, last_online)"""

sns.histplot(x='gender', data=users, palette="Set3", hue="gender")
plt.title("Jumlah pengguna berdasarkan gender")
plt.show()

"""Distribusi gender users:  
F: Female  
M: Male  
U: Unknown (tidak memilih gender)
"""

duplicates_mask = users['uid'].duplicated()
uid_with_duplicates = users['uid'][duplicates_mask]

print(uid_with_duplicates)
print(f"Banyak duplicates: {len(uid_with_duplicates)}")

gender_counts = users['gender'].value_counts()

plt.pie(gender_counts, labels=gender_counts.index, autopct="%1.1f%%")
plt.title("Persentasi gender pengguna mydramalist.com")
plt.show()

"""Tidak ada data duplikat pada users

### pairs/rating
"""

pairs.head()

pairs.info()

"""Terdapat 5 data numerik (score, sid, uid, ep_seen, ep_total) dan 1 kategorikal (completed)"""

pairs.isnull().sum()

"""Tidak terdapat null dalam dataset"""

pairs.describe()

"""Rating minimum = 0  
Rating maximum = 10  
Mean Rating = 6.282575

Banyak data setelah dipreprocess:
"""

sns.histplot(pairs['score'], bins=10, kde=True, color='skyblue', edgecolor='black')

plt.title('Distribusi Rating shows mydramalist.com')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

"""Rating banyak terdistribusi pada rating 7-10 dan 0.

Banyak data setelah dipreprocess
"""

print(f"banyak data pairs {len(pairs)}")
print(f"banyak data shows {len(shows)}")
print(f"banyak data users {len(users)}")

"""## Content-Based FIltering

### Data Preprocessing
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()

data = shows.loc[:10000, 'kind country']
tf.fit(data)

tf.get_feature_names_out()

tfidf_matrix = tf.fit_transform(data)

tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=shows.loc[:10000, 'title']
).sample(8)

sparse_tfidf_matrix = csr_matrix(tfidf_matrix)

"""Digunakan sparse matrix agar ram tidak overflow

### Cosine Similarity
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(sparse_tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=shows.loc[:10000, 'title'], columns=shows.loc[:10000, 'title'])
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### Top N recommendations"""

def shows_recommendations(nama_shows, similarity_data=cosine_sim_df, items=shows.loc[:10000, ['title', 'kind country']], k=5):

    index = similarity_data.loc[:,nama_shows].to_numpy().argpartition(
        range(-1, -k, -1))

    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    closest = closest.drop(nama_shows, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

shows_recommendations('Melting Me Softly')

"""## Collaborative Filtering

### Data Preprocessing
"""

df = pairs

user_ids = df["uid"].unique().tolist()
user2user_encoded = {x: i for i, x in enumerate(user_ids)}
userencoded2user = {i: x for i, x in enumerate(user_ids)}
movie_ids = df["sid"].unique().tolist()
movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}
movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}
df["user"] = df["uid"].map(user2user_encoded)
df["shows"] = df["sid"].map(movie2movie_encoded)

num_users = len(user2user_encoded)
num_movies = len(movie_encoded2movie)
df["score"] = df["score"].values.astype(np.float32)

min_rating = min(df["score"])
max_rating = max(df["score"])

print(
    "Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}".format(
        num_users, num_movies, min_rating, max_rating
    )
)

df = df.sample(frac=1, random_state=42)
x = df[["user", "shows"]].values
y = df["score"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.9 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:],
)

"""### Building RecommenderNet"""

class RecommenderNet(tf.keras.Model):

  def __init__(self, num_users, num_shows, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_shows = num_shows
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( 
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) 
    self.show_embedding = layers.Embedding( 
        num_shows,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.show_bias = layers.Embedding(num_shows, 1) 

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) 
    user_bias = self.user_bias(inputs[:, 0]) 
    show_vector = self.show_embedding(inputs[:, 1]) 
    show_bias = self.show_bias(inputs[:, 1]) 

    dot_user_show = tf.tensordot(user_vector, show_vector, 2)

    x = dot_user_show + user_bias + show_bias

    return tf.nn.sigmoid(x) 

model = RecommenderNet(num_users, num_movies, 200) 

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Nadam(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

target_rmse = 0.27

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('root_mean_squared_error') < target_rmse and logs.get('val_root_mean_squared_error') < target_rmse):
      print(f"\RMSE telah mencapai < {target_rmse}!")
      self.model.stop_training = True
callbacks = myCallback()

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 512,
    epochs = 10,
    validation_data = (x_val, y_val),
    callbacks=[callbacks]
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Top N Recommendations"""

user_id = df['uid'].sample(1).iloc[0]
shows_visited_by_user = df[df['uid'] == user_id]

shows_not_visited = shows[~shows['sid'].isin(shows_visited_by_user['sid'].values)]['sid']
shows_not_visited = list(
    set(shows_not_visited)
    .intersection(set(movie2movie_encoded.keys()))
)

shows_not_visited = [[movie2movie_encoded.get(x)] for x in shows_not_visited]
user_encoder = user2user_encoded.get(user_id)
user_shows_array = np.hstack(
    ([[user_encoder]] * len(shows_not_visited), shows_not_visited)
)

ratings = model.predict(user_shows_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_shows_ids = [
    movie2movie_encoded.get(shows_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('shows with high ratings from user')
print('----' * 8)

top_shows_user = (
    shows_visited_by_user.sort_values(
        by = 'score',
        ascending=False
    )
    .head(5)
    ['sid'].values
)

shows_df_rows = shows[shows['sid'].isin(top_shows_user)]
print(shows_df_rows[['title', 'kind']])

print('----' * 8)
print('Top 10 shows recommendation')
print('----' * 8)

recommended_shows = shows[shows['sid'].isin(recommended_shows_ids)]
print(recommended_shows[['title', 'kind']])